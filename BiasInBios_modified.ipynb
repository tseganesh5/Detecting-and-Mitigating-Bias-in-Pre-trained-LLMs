{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Set a fixed seed for reproducibility\n",
    "SEED = 2024\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)  # For multi-GPU setups\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribution of Profession Classes:\n",
      "Counter({21: 76748, 19: 26648, 2: 21169, 18: 15773, 11: 12960, 13: 12316, 22: 11945, 26: 10531, 6: 9479, 25: 8829, 1: 6568, 14: 5025, 12: 4867, 20: 4558, 9: 4545, 24: 4492, 0: 3660, 5: 3637, 7: 2567, 4: 1824, 3: 1725, 16: 1638, 15: 1146, 27: 1076, 8: 964, 10: 949, 17: 928, 23: 911})\n",
      "\n",
      "Distribution of Gender Classes:\n",
      "Counter({0: 138780, 1: 118698})\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "train_dataset_bib = load_dataset(\"LabHC/bias_in_bios\", split='train')\n",
    "test_dataset_bib = load_dataset(\"LabHC/bias_in_bios\", split='test')\n",
    "dev_dataset_bib = load_dataset(\"LabHC/bias_in_bios\", split='dev')\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Count distributions for profession and gender\n",
    "profession_labels = [sample['profession'] for sample in train_dataset_bib]\n",
    "gender_labels = [sample['gender'] for sample in train_dataset_bib]\n",
    "\n",
    "# Profession distribution\n",
    "profession_distribution = Counter(profession_labels)\n",
    "print(\"\\nDistribution of Profession Classes:\")\n",
    "print(profession_distribution)\n",
    "\n",
    "# Gender distribution\n",
    "gender_distribution = Counter(gender_labels)\n",
    "print(\"\\nDistribution of Gender Classes:\")\n",
    "print(gender_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample from Training Data:\n",
      "Text: He is also the project lead of and major contributor to the open source assembler/simulator \"EASy68K.\" He earned a master’s degree in computer science from the University of Michigan-Dearborn, where he is also an adjunct instructor. Downloads/Updates\n",
      "Profession (label): 21\n",
      "Gender (sensitive attribute): 0\n",
      "\n",
      "Dataset Sizes:\n",
      "Train: 257478\n",
      "Test: 99069\n",
      "Dev: 39642\n"
     ]
    }
   ],
   "source": [
    "# Extract the necessary columns and create a simplified dataset\n",
    "train_texts = [sample['hard_text'] for sample in train_dataset_bib]\n",
    "train_professions = [sample['profession'] for sample in train_dataset_bib]\n",
    "train_genders = [sample['gender'] for sample in train_dataset_bib]\n",
    "\n",
    "test_texts = [sample['hard_text'] for sample in test_dataset_bib]\n",
    "test_professions = [sample['profession'] for sample in test_dataset_bib]\n",
    "test_genders = [sample['gender'] for sample in test_dataset_bib]\n",
    "\n",
    "dev_texts = [sample['hard_text'] for sample in dev_dataset_bib]\n",
    "dev_professions = [sample['profession'] for sample in dev_dataset_bib]\n",
    "dev_genders = [sample['gender'] for sample in dev_dataset_bib]\n",
    "\n",
    "# Print a few samples to confirm preprocessing\n",
    "print(\"\\nSample from Training Data:\")\n",
    "print(\"Text:\", train_texts[0])\n",
    "print(\"Profession (label):\", train_professions[0])\n",
    "print(\"Gender (sensitive attribute):\", train_genders[0])\n",
    "\n",
    "# Confirm dataset sizes\n",
    "print(\"\\nDataset Sizes:\")\n",
    "print(\"Train:\", len(train_texts))\n",
    "print(\"Test:\", len(test_texts))\n",
    "print(\"Dev:\", len(dev_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gender Proportion by Profession:\n",
      "gender             0         1\n",
      "profession                    \n",
      "0           0.633060  0.366940\n",
      "1           0.763398  0.236602\n",
      "2           0.617129  0.382871\n",
      "3           0.736812  0.263188\n",
      "4           0.788925  0.211075\n",
      "5           0.836404  0.163596\n",
      "6           0.647009  0.352991\n",
      "7           0.071289  0.928711\n",
      "8           0.858921  0.141079\n",
      "9           0.670627  0.329373\n",
      "10          0.191781  0.808219\n",
      "11          0.505015  0.494985\n",
      "12          0.172591  0.827409\n",
      "13          0.091507  0.908493\n",
      "14          0.542687  0.457313\n",
      "15          0.150960  0.849040\n",
      "16          0.760073  0.239927\n",
      "17          0.544181  0.455819\n",
      "18          0.642934  0.357066\n",
      "19          0.506304  0.493696\n",
      "20          0.509653  0.490347\n",
      "21          0.548939  0.451061\n",
      "22          0.379238  0.620762\n",
      "23          0.903403  0.096597\n",
      "24          0.842164  0.157836\n",
      "25          0.851852  0.148148\n",
      "26          0.397683  0.602317\n",
      "27          0.154275  0.845725\n",
      "\n",
      "Top 5 Male-Dominated Professions:\n",
      "profession\n",
      "23    0.903403\n",
      "8     0.858921\n",
      "25    0.851852\n",
      "24    0.842164\n",
      "5     0.836404\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "Top 5 Female-Dominated Professions:\n",
      "profession\n",
      "7     0.928711\n",
      "13    0.908493\n",
      "15    0.849040\n",
      "27    0.845725\n",
      "12    0.827409\n",
      "Name: 1, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame for easier analysis\n",
    "train_data = pd.DataFrame({\n",
    "    \"text\": train_texts,\n",
    "    \"profession\": train_professions,\n",
    "    \"gender\": train_genders\n",
    "})\n",
    "\n",
    "# Calculate gender distribution for each profession\n",
    "profession_gender_dist = train_data.groupby(\"profession\")[\"gender\"].value_counts(normalize=True).unstack()\n",
    "\n",
    "# Display the proportion of male (0) and female (1) samples for each profession\n",
    "print(\"\\nGender Proportion by Profession:\")\n",
    "print(profession_gender_dist)\n",
    "\n",
    "# Identify professions with the largest gender imbalance\n",
    "most_male_dominated = profession_gender_dist[0].sort_values(ascending=False).head(5)\n",
    "most_female_dominated = profession_gender_dist[1].sort_values(ascending=False).head(5)\n",
    "\n",
    "print(\"\\nTop 5 Male-Dominated Professions:\")\n",
    "print(most_male_dominated)\n",
    "\n",
    "print(\"\\nTop 5 Female-Dominated Professions:\")\n",
    "print(most_female_dominated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Batch from Train Loader:\n",
      "Input IDs: torch.Size([16, 128])\n",
      "Attention Mask: torch.Size([16, 128])\n",
      "Labels: torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    # Ensure workers use the same seed\n",
    "    worker_seed = SEED + worker_id\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "# Load pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Define a custom PyTorch dataset class\n",
    "class BiasInBiosDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenize the text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Return input IDs, attention mask, and label\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "# Define datasets for training, testing, and validation\n",
    "train_dataset = BiasInBiosDataset(train_texts, train_professions, tokenizer)\n",
    "test_dataset = BiasInBiosDataset(test_texts, test_professions, tokenizer)\n",
    "dev_dataset = BiasInBiosDataset(dev_texts, dev_professions, tokenizer)\n",
    "\n",
    "# Define data loaders for batching\n",
    "# Create DataLoader with a fixed seed\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=torch.Generator().manual_seed(SEED)  # Ensure reproducibility in DataLoader\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Verify preprocessing with a sample batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(\"\\nSample Batch from Train Loader:\")\n",
    "print(\"Input IDs:\", sample_batch[\"input_ids\"].shape)\n",
    "print(\"Attention Mask:\", sample_batch[\"attention_mask\"].shape)\n",
    "print(\"Labels:\", sample_batch[\"label\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Loaded and Ready for Training!\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the BERT-based classifier\n",
    "class BertProfessionClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BertProfessionClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Pass input through BERT\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Use [CLS] token representation (hidden state of the first token)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        # Apply dropout and classification head\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Instantiate the model\n",
    "num_classes = 28  # Number of profession labels\n",
    "model = BertProfessionClassifier(num_classes)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"\\nModel Loaded and Ready for Training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Hyperparameter Code (Optional, code with multiple hyperparameters is under this code block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Training parameters\n",
    "epochs = 3\n",
    "learning_rate = 2e-5\n",
    "batch_size = 16\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "    for batch in tqdm(train_loader):\n",
    "        # Move data to the same device as the model\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        # Forward pass and compute loss\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    # Scheduler step\n",
    "    scheduler.step()\n",
    "\n",
    "    # Epoch results\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Training Loss: {avg_loss:.4f}, Training Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nTraining Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ryang\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with learning_rate=1e-05, batch_size=8, epochs=3\n",
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32185/32185 [24:43<00:00, 21.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss: 0.5834, Training Accuracy: 0.8329\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32185/32185 [24:44<00:00, 21.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss: 0.3610, Training Accuracy: 0.8906\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32185/32185 [24:44<00:00, 21.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss: 0.3277, Training Accuracy: 0.9008\n",
      "Results: Learning Rate: 1e-05, Batch Size: 8, Epochs: 3, Test Accuracy: 0.8665\n",
      "\n",
      "Training with learning_rate=1e-05, batch_size=8, epochs=5\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32185/32185 [24:43<00:00, 21.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss: 0.3716, Training Accuracy: 0.8875\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32185/32185 [24:43<00:00, 21.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss: 0.2397, Training Accuracy: 0.9271\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32185/32185 [24:43<00:00, 21.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss: 0.2079, Training Accuracy: 0.9370\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32185/32185 [24:43<00:00, 21.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Training Loss: 0.2046, Training Accuracy: 0.9381\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32185/32185 [24:42<00:00, 21.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Training Loss: 0.2046, Training Accuracy: 0.9382\n",
      "Results: Learning Rate: 1e-05, Batch Size: 8, Epochs: 5, Test Accuracy: 0.8656\n",
      "\n",
      "Training with learning_rate=1e-05, batch_size=16, epochs=3\n",
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16093/16093 [16:44<00:00, 16.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss: 0.2325, Training Accuracy: 0.9290\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16093/16093 [16:36<00:00, 16.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss: 0.1492, Training Accuracy: 0.9544\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16093/16093 [16:36<00:00, 16.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss: 0.1281, Training Accuracy: 0.9617\n",
      "Results: Learning Rate: 1e-05, Batch Size: 16, Epochs: 3, Test Accuracy: 0.8622\n",
      "\n",
      "Training with learning_rate=1e-05, batch_size=16, epochs=5\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16093/16093 [16:36<00:00, 16.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss: 0.1550, Training Accuracy: 0.9521\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16093/16093 [16:35<00:00, 16.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss: 0.0966, Training Accuracy: 0.9703\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16093/16093 [16:35<00:00, 16.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss: 0.0777, Training Accuracy: 0.9768\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16093/16093 [16:35<00:00, 16.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Training Loss: 0.0768, Training Accuracy: 0.9771\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16093/16093 [16:35<00:00, 16.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Training Loss: 0.0759, Training Accuracy: 0.9774\n",
      "Results: Learning Rate: 1e-05, Batch Size: 16, Epochs: 5, Test Accuracy: 0.8592\n",
      "\n",
      "Training with learning_rate=1e-05, batch_size=32, epochs=3\n",
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8047/8047 [14:32<00:00,  9.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss: 0.0935, Training Accuracy: 0.9707\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8047/8047 [14:31<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss: 0.0599, Training Accuracy: 0.9816\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8047/8047 [14:29<00:00,  9.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss: 0.0490, Training Accuracy: 0.9856\n",
      "Results: Learning Rate: 1e-05, Batch Size: 32, Epochs: 3, Test Accuracy: 0.8577\n",
      "\n",
      "Training with learning_rate=1e-05, batch_size=32, epochs=5\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8047/8047 [14:43<00:00,  9.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss: 0.0658, Training Accuracy: 0.9794\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8047/8047 [14:44<00:00,  9.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss: 0.0432, Training Accuracy: 0.9863\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8047/8047 [14:34<00:00,  9.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss: 0.0339, Training Accuracy: 0.9897\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8047/8047 [14:30<00:00,  9.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Training Loss: 0.0329, Training Accuracy: 0.9902\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8047/8047 [14:30<00:00,  9.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Training Loss: 0.0327, Training Accuracy: 0.9902\n",
      "Results: Learning Rate: 1e-05, Batch Size: 32, Epochs: 5, Test Accuracy: 0.8567\n",
      "\n",
      "Training with learning_rate=2e-05, batch_size=8, epochs=3\n",
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32185/32185 [24:43<00:00, 21.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss: 0.1899, Training Accuracy: 0.9385\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32185/32185 [24:43<00:00, 21.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss: 0.0906, Training Accuracy: 0.9705\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32185/32185 [24:43<00:00, 21.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss: 0.0464, Training Accuracy: 0.9857\n",
      "Results: Learning Rate: 2e-05, Batch Size: 8, Epochs: 3, Test Accuracy: 0.8555\n",
      "\n",
      "Training with learning_rate=2e-05, batch_size=8, epochs=5\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32185/32185 [24:43<00:00, 21.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss: 0.1555, Training Accuracy: 0.9488\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32185/32185 [24:43<00:00, 21.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss: 0.0702, Training Accuracy: 0.9773\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32185/32185 [24:43<00:00, 21.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss: 0.0343, Training Accuracy: 0.9894\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32185/32185 [24:43<00:00, 21.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Training Loss: 0.0304, Training Accuracy: 0.9907\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32185/32185 [24:43<00:00, 21.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Training Loss: 0.0308, Training Accuracy: 0.9905\n",
      "Results: Learning Rate: 2e-05, Batch Size: 8, Epochs: 5, Test Accuracy: 0.8557\n",
      "\n",
      "Training with learning_rate=2e-05, batch_size=16, epochs=3\n",
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16093/16093 [16:36<00:00, 16.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss: 0.0853, Training Accuracy: 0.9718\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16093/16093 [16:35<00:00, 16.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss: 0.0410, Training Accuracy: 0.9867\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16093/16093 [16:35<00:00, 16.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss: 0.0214, Training Accuracy: 0.9934\n",
      "Results: Learning Rate: 2e-05, Batch Size: 16, Epochs: 3, Test Accuracy: 0.8553\n",
      "\n",
      "Training with learning_rate=2e-05, batch_size=16, epochs=5\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16093/16093 [16:34<00:00, 16.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss: 0.0687, Training Accuracy: 0.9772\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16093/16093 [16:35<00:00, 16.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss: 0.0327, Training Accuracy: 0.9893\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16093/16093 [16:35<00:00, 16.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss: 0.0154, Training Accuracy: 0.9954\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16093/16093 [16:35<00:00, 16.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Training Loss: 0.0143, Training Accuracy: 0.9957\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16093/16093 [16:35<00:00, 16.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Training Loss: 0.0140, Training Accuracy: 0.9958\n",
      "Results: Learning Rate: 2e-05, Batch Size: 16, Epochs: 5, Test Accuracy: 0.8540\n",
      "\n",
      "Training with learning_rate=2e-05, batch_size=32, epochs=3\n",
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8047/8047 [14:29<00:00,  9.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss: 0.0398, Training Accuracy: 0.9867\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8047/8047 [14:28<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss: 0.0201, Training Accuracy: 0.9935\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8047/8047 [14:28<00:00,  9.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss: 0.0111, Training Accuracy: 0.9966\n",
      "Results: Learning Rate: 2e-05, Batch Size: 32, Epochs: 3, Test Accuracy: 0.8551\n",
      "\n",
      "Training with learning_rate=2e-05, batch_size=32, epochs=5\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8047/8047 [14:29<00:00,  9.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss: 0.0334, Training Accuracy: 0.9890\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8047/8047 [14:28<00:00,  9.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss: 0.0160, Training Accuracy: 0.9948\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8047/8047 [14:27<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss: 0.0084, Training Accuracy: 0.9974\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8047/8047 [14:28<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Training Loss: 0.0079, Training Accuracy: 0.9977\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8047/8047 [14:29<00:00,  9.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Training Loss: 0.0080, Training Accuracy: 0.9977\n",
      "Results: Learning Rate: 2e-05, Batch Size: 32, Epochs: 5, Test Accuracy: 0.8535\n",
      "\n",
      "Training with learning_rate=5e-05, batch_size=8, epochs=3\n",
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32185/32185 [24:44<00:00, 21.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss: 0.3631, Training Accuracy: 0.8895\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32185/32185 [24:43<00:00, 21.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss: 0.1924, Training Accuracy: 0.9388\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32185/32185 [24:43<00:00, 21.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss: 0.0850, Training Accuracy: 0.9735\n",
      "Results: Learning Rate: 5e-05, Batch Size: 8, Epochs: 3, Test Accuracy: 0.8482\n",
      "\n",
      "Training with learning_rate=5e-05, batch_size=8, epochs=5\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32185/32185 [24:44<00:00, 21.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss: 0.3707, Training Accuracy: 0.8867\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32185/32185 [24:43<00:00, 21.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss: 0.1849, Training Accuracy: 0.9419\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32185/32185 [24:44<00:00, 21.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss: 0.0943, Training Accuracy: 0.9707\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32185/32185 [24:42<00:00, 21.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Training Loss: 0.0844, Training Accuracy: 0.9735\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32185/32185 [24:43<00:00, 21.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Training Loss: 0.0828, Training Accuracy: 0.9743\n",
      "Results: Learning Rate: 5e-05, Batch Size: 8, Epochs: 5, Test Accuracy: 0.8428\n",
      "\n",
      "Training with learning_rate=5e-05, batch_size=16, epochs=3\n",
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16093/16093 [16:34<00:00, 16.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss: 0.2288, Training Accuracy: 0.9285\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16093/16093 [16:34<00:00, 16.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss: 0.1051, Training Accuracy: 0.9671\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16093/16093 [16:33<00:00, 16.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss: 0.0541, Training Accuracy: 0.9835\n",
      "Results: Learning Rate: 5e-05, Batch Size: 16, Epochs: 3, Test Accuracy: 0.8405\n",
      "\n",
      "Training with learning_rate=5e-05, batch_size=16, epochs=5\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16093/16093 [16:33<00:00, 16.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss: 0.1650, Training Accuracy: 0.9476\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16093/16093 [16:34<00:00, 16.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss: 0.0757, Training Accuracy: 0.9759\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16093/16093 [16:33<00:00, 16.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss: 0.0343, Training Accuracy: 0.9896\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16093/16093 [16:34<00:00, 16.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Training Loss: 0.0307, Training Accuracy: 0.9907\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16093/16093 [16:33<00:00, 16.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Training Loss: 0.0299, Training Accuracy: 0.9909\n",
      "Results: Learning Rate: 5e-05, Batch Size: 16, Epochs: 5, Test Accuracy: 0.8384\n",
      "\n",
      "Training with learning_rate=5e-05, batch_size=32, epochs=3\n",
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8047/8047 [14:29<00:00,  9.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss: 0.0944, Training Accuracy: 0.9698\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8047/8047 [14:34<00:00,  9.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss: 0.0408, Training Accuracy: 0.9871\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8047/8047 [14:33<00:00,  9.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss: 0.0195, Training Accuracy: 0.9941\n",
      "Results: Learning Rate: 5e-05, Batch Size: 32, Epochs: 3, Test Accuracy: 0.8386\n",
      "\n",
      "Training with learning_rate=5e-05, batch_size=32, epochs=5\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8047/8047 [14:26<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss: 0.0732, Training Accuracy: 0.9765\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8047/8047 [14:28<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss: 0.0347, Training Accuracy: 0.9890\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8047/8047 [14:27<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss: 0.0160, Training Accuracy: 0.9951\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8047/8047 [14:28<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Training Loss: 0.0139, Training Accuracy: 0.9958\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8047/8047 [14:27<00:00,  9.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Training Loss: 0.0137, Training Accuracy: 0.9958\n",
      "Results: Learning Rate: 5e-05, Batch Size: 32, Epochs: 5, Test Accuracy: 0.8375\n",
      "\n",
      "All Hyperparameter Tuning Results:\n",
      "{'learning_rate': 1e-05, 'batch_size': 8, 'epochs': 3, 'training_loss': 0.3276983403063256, 'test_accuracy': 0.8665475577627714}\n",
      "{'learning_rate': 1e-05, 'batch_size': 8, 'epochs': 5, 'training_loss': 0.20464519201786918, 'test_accuracy': 0.8655785361717591}\n",
      "{'learning_rate': 1e-05, 'batch_size': 16, 'epochs': 3, 'training_loss': 0.12810454363124932, 'test_accuracy': 0.8622475244526542}\n",
      "{'learning_rate': 1e-05, 'batch_size': 16, 'epochs': 5, 'training_loss': 0.07592265650379039, 'test_accuracy': 0.8591991440309279}\n",
      "{'learning_rate': 1e-05, 'batch_size': 32, 'epochs': 3, 'training_loss': 0.04898987576431209, 'test_accuracy': 0.8577153297196903}\n",
      "{'learning_rate': 1e-05, 'batch_size': 32, 'epochs': 5, 'training_loss': 0.032721297986310904, 'test_accuracy': 0.8566756503043333}\n",
      "{'learning_rate': 2e-05, 'batch_size': 8, 'epochs': 3, 'training_loss': 0.04638274693285631, 'test_accuracy': 0.8554643733155679}\n",
      "{'learning_rate': 2e-05, 'batch_size': 8, 'epochs': 5, 'training_loss': 0.03082715360430285, 'test_accuracy': 0.8557369106380401}\n",
      "{'learning_rate': 2e-05, 'batch_size': 16, 'epochs': 3, 'training_loss': 0.02144757726834938, 'test_accuracy': 0.8552927757421595}\n",
      "{'learning_rate': 2e-05, 'batch_size': 16, 'epochs': 5, 'training_loss': 0.01399165940822084, 'test_accuracy': 0.8540007469541431}\n",
      "{'learning_rate': 2e-05, 'batch_size': 32, 'epochs': 3, 'training_loss': 0.011102933043549609, 'test_accuracy': 0.8551009902189383}\n",
      "{'learning_rate': 2e-05, 'batch_size': 32, 'epochs': 5, 'training_loss': 0.007993544798365212, 'test_accuracy': 0.8534960482088242}\n",
      "{'learning_rate': 5e-05, 'batch_size': 8, 'epochs': 3, 'training_loss': 0.08500828971188808, 'test_accuracy': 0.8482269933076947}\n",
      "{'learning_rate': 5e-05, 'batch_size': 8, 'epochs': 5, 'training_loss': 0.08281463371755333, 'test_accuracy': 0.8428065287829695}\n",
      "{'learning_rate': 5e-05, 'batch_size': 16, 'epochs': 3, 'training_loss': 0.054053679456965036, 'test_accuracy': 0.8404546326297833}\n",
      "{'learning_rate': 5e-05, 'batch_size': 16, 'epochs': 5, 'training_loss': 0.029928983621062603, 'test_accuracy': 0.8383550858492566}\n",
      "{'learning_rate': 5e-05, 'batch_size': 32, 'epochs': 3, 'training_loss': 0.019461075047214154, 'test_accuracy': 0.8386377171466352}\n",
      "{'learning_rate': 5e-05, 'batch_size': 32, 'epochs': 5, 'training_loss': 0.013742832739552933, 'test_accuracy': 0.8374567220825889}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define hyperparameter combinations\n",
    "learning_rates = [1e-5, 2e-5, 5e-5]\n",
    "batch_sizes = [8, 16, 32]\n",
    "epochs_list = [3, 5]\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        for epochs in epochs_list:\n",
    "            print(f\"\\nTraining with learning_rate={lr}, batch_size={batch_size}, epochs={epochs}\")\n",
    "            \n",
    "            # Update DataLoader with new batch size\n",
    "            train_loader = DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                worker_init_fn=seed_worker,\n",
    "                generator=torch.Generator().manual_seed(SEED)\n",
    "            )\n",
    "            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            # Define optimizer with new learning rate\n",
    "            optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "            # Define loss function\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "            # Learning rate scheduler\n",
    "            scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "            # Training loop\n",
    "            for epoch in range(epochs):\n",
    "                model.train()\n",
    "                total_loss = 0\n",
    "                correct = 0\n",
    "                total = 0\n",
    "\n",
    "                print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "                for batch in tqdm(train_loader):\n",
    "                    # Move data to the same device as the model\n",
    "                    input_ids = batch[\"input_ids\"].to(device)\n",
    "                    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                    labels = batch[\"label\"].to(device)\n",
    "\n",
    "                    # Forward pass and compute loss\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(input_ids, attention_mask)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    total_loss += loss.item()\n",
    "\n",
    "                    # Backward pass and optimization\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # Compute accuracy\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                    total += labels.size(0)\n",
    "\n",
    "                # Scheduler step\n",
    "                scheduler.step()\n",
    "\n",
    "                # Epoch results\n",
    "                avg_loss = total_loss / len(train_loader)\n",
    "                accuracy = correct / total\n",
    "                print(f\"Epoch {epoch+1}: Training Loss: {avg_loss:.4f}, Training Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "            # Evaluate model on test set\n",
    "            model.eval()\n",
    "            test_correct = 0\n",
    "            test_total = 0\n",
    "            with torch.no_grad():\n",
    "                for batch in test_loader:\n",
    "                    input_ids = batch[\"input_ids\"].to(device)\n",
    "                    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                    labels = batch[\"label\"].to(device)\n",
    "\n",
    "                    outputs = model(input_ids, attention_mask)\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    test_correct += (predicted == labels).sum().item()\n",
    "                    test_total += labels.size(0)\n",
    "\n",
    "            test_accuracy = test_correct / test_total\n",
    "\n",
    "            # Record results\n",
    "            results.append({\n",
    "                \"learning_rate\": lr,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"epochs\": epochs,\n",
    "                \"training_loss\": avg_loss,\n",
    "                \"test_accuracy\": test_accuracy\n",
    "            })\n",
    "            print(f\"Results: Learning Rate: {lr}, Batch Size: {batch_size}, Epochs: {epochs}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Print all results\n",
    "print(\"\\nAll Hyperparameter Tuning Results:\")\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Define the save directory\n",
    "save_directory = \"./bert_profession_classifier\"\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Save the model's state dict\n",
    "torch.save(model.state_dict(), os.path.join(save_directory, \"model.pth\"))\n",
    "\n",
    "# Save the tokenizer (if applicable)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {save_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryang\\AppData\\Local\\Temp\\ipykernel_24296\\4258747354.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(os.path.join(save_directory, \"model.pth\")))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer successfully loaded!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Reinitialize the model\n",
    "save_directory = \"./bert_profession_classifier\"\n",
    "model = BertProfessionClassifier(num_classes)\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load(os.path.join(save_directory, \"model.pth\")))\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "model.to(device)\n",
    "\n",
    "# Load the tokenizer (if applicable)\n",
    "tokenizer = BertTokenizer.from_pretrained(save_directory)\n",
    "\n",
    "print(\"Model and tokenizer successfully loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6192/6192 [02:48<00:00, 36.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8353    0.7700    0.8013      1409\n",
      "           1     0.7942    0.7112    0.7504      2528\n",
      "           2     0.9001    0.9215    0.9107      8143\n",
      "           3     0.8027    0.7108    0.7540       664\n",
      "           4     0.8301    0.8407    0.8353       703\n",
      "           5     0.8601    0.9044    0.8817      1401\n",
      "           6     0.9410    0.9580    0.9495      3647\n",
      "           7     0.8657    0.8665    0.8661       989\n",
      "           8     0.8790    0.8199    0.8484       372\n",
      "           9     0.8550    0.8629    0.8589      1750\n",
      "          10     0.7994    0.7514    0.7746       366\n",
      "          11     0.7895    0.8191    0.8040      4986\n",
      "          12     0.8603    0.8218    0.8406      1874\n",
      "          13     0.8651    0.8499    0.8574      4738\n",
      "          14     0.8478    0.8605    0.8541      1935\n",
      "          15     0.8567    0.7036    0.7727       442\n",
      "          16     0.6956    0.7484    0.7210       632\n",
      "          17     0.7413    0.7765    0.7585       358\n",
      "          18     0.8939    0.8998    0.8968      6068\n",
      "          19     0.8782    0.8798    0.8790     10250\n",
      "          20     0.8055    0.8170    0.8112      1754\n",
      "          21     0.9090    0.9357    0.9221     29520\n",
      "          22     0.8310    0.7844    0.8070      4596\n",
      "          23     0.8494    0.8519    0.8506       351\n",
      "          24     0.7634    0.7766    0.7699      1728\n",
      "          25     0.8302    0.7266    0.7750      3398\n",
      "          26     0.7076    0.6690    0.6877      4051\n",
      "          27     0.7432    0.7861    0.7640       416\n",
      "\n",
      "    accuracy                         0.8663     99069\n",
      "   macro avg     0.8297    0.8151    0.8215     99069\n",
      "weighted avg     0.8652    0.8663    0.8653     99069\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluation loop\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader):\n",
    "            # Move data to the same device as the model\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            # Collect predictions and true labels\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return predictions, true_labels\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "predictions, true_labels = evaluate_model(model, test_loader)\n",
    "\n",
    "# Generate a classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predictions, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bias Detection Results:\n",
      "Male Prompt: He is a teacher. -> Predicted: teacher\n",
      "Female Prompt: She is a teacher. -> Predicted: teacher\n",
      "--------------------------------------------------\n",
      "Male Prompt: He is a nurse. -> Predicted: nurse\n",
      "Female Prompt: She is a nurse. -> Predicted: nurse\n",
      "--------------------------------------------------\n",
      "Male Prompt: He is a software engineer. -> Predicted: software_engineer\n",
      "Female Prompt: She is a software engineer. -> Predicted: software_engineer\n",
      "--------------------------------------------------\n",
      "Male Prompt: He is a surgeon. -> Predicted: surgeon\n",
      "Female Prompt: She is a surgeon. -> Predicted: professor\n",
      "--------------------------------------------------\n",
      "Male Prompt: He is a rapper. -> Predicted: rapper\n",
      "Female Prompt: She is a rapper. -> Predicted: rapper\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Controlled prompts for bias detection\n",
    "male_prompts = [f\"He is a {profession}.\" for profession in [\n",
    "    \"teacher\", \"nurse\", \"software engineer\", \"surgeon\", \"rapper\"\n",
    "]]\n",
    "female_prompts = [f\"She is a {profession}.\" for profession in [\n",
    "    \"teacher\", \"nurse\", \"software engineer\", \"surgeon\", \"rapper\"\n",
    "]]\n",
    "\n",
    "# Tokenize prompts\n",
    "male_inputs = tokenizer(male_prompts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "female_inputs = tokenizer(female_prompts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Get model predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    male_outputs = model(male_inputs[\"input_ids\"], male_inputs[\"attention_mask\"])\n",
    "    female_outputs = model(female_inputs[\"input_ids\"], female_inputs[\"attention_mask\"])\n",
    "\n",
    "# Convert predictions to probabilities\n",
    "male_probs = torch.nn.functional.softmax(male_outputs, dim=1)\n",
    "female_probs = torch.nn.functional.softmax(female_outputs, dim=1)\n",
    "\n",
    "# Get top predicted classes and probabilities\n",
    "male_predictions = torch.argmax(male_probs, dim=1).cpu().numpy()\n",
    "female_predictions = torch.argmax(female_probs, dim=1).cpu().numpy()\n",
    "\n",
    "# Map profession labels back to their names\n",
    "profession_mapping = {\n",
    "    0: \"accountant\", 1: \"architect\", 2: \"attorney\", 3: \"chiropractor\", 4: \"comedian\",\n",
    "    5: \"composer\", 6: \"dentist\", 7: \"dietitian\", 8: \"dj\", 9: \"filmmaker\", 10: \"interior_designer\",\n",
    "    11: \"journalist\", 12: \"model\", 13: \"nurse\", 14: \"painter\", 15: \"paralegal\", 16: \"pastor\",\n",
    "    17: \"personal_trainer\", 18: \"photographer\", 19: \"physician\", 20: \"poet\", 21: \"professor\",\n",
    "    22: \"psychologist\", 23: \"rapper\", 24: \"software_engineer\", 25: \"surgeon\", 26: \"teacher\",\n",
    "    27: \"yoga_teacher\"\n",
    "}\n",
    "\n",
    "# Print predictions for each prompt\n",
    "print(\"\\nBias Detection Results:\")\n",
    "for i in range(len(male_prompts)):\n",
    "    print(f\"Male Prompt: {male_prompts[i]} -> Predicted: {profession_mapping[male_predictions[i]]}\")\n",
    "    print(f\"Female Prompt: {female_prompts[i]} -> Predicted: {profession_mapping[female_predictions[i]]}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bias Scores:\n",
      "Profession: teacher - Bias Score: 0.0185\n",
      "Profession: nurse - Bias Score: 0.3243\n",
      "Profession: engineer - Bias Score: 0.0957\n",
      "Profession: surgeon - Bias Score: 0.0748\n",
      "Profession: rapper - Bias Score: 0.3775\n"
     ]
    }
   ],
   "source": [
    "# Calculate bias scores\n",
    "bias_scores = []\n",
    "for i in range(len(male_prompts)):\n",
    "    male_prob = male_probs[i, male_predictions[i]].item()\n",
    "    female_prob = female_probs[i, female_predictions[i]].item()\n",
    "    bias_scores.append(abs(male_prob - female_prob))\n",
    "\n",
    "# Print bias scores for each profession\n",
    "print(\"\\nBias Scores:\")\n",
    "for i in range(len(male_prompts)):\n",
    "    print(f\"Profession: {male_prompts[i].split()[-1][:-1]} - Bias Score: {bias_scores[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balancing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Balanced Training Dataset Created!\n",
      "Total Samples: 2359280\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Combine the training data into a DataFrame for processing\n",
    "train_data = pd.DataFrame({\n",
    "    \"text\": train_texts,\n",
    "    \"profession\": train_professions,\n",
    "    \"gender\": train_genders\n",
    "})\n",
    "\n",
    "# Count the gender distribution for each profession\n",
    "gender_profession_counts = train_data.groupby([\"profession\", \"gender\"]).size()\n",
    "\n",
    "# Find the maximum number of samples for any gender-profession combination\n",
    "max_samples = gender_profession_counts.max()\n",
    "\n",
    "# Oversample and undersample the data\n",
    "balanced_data = []\n",
    "for profession in train_data[\"profession\"].unique():\n",
    "    for gender in [0, 1]:  # 0 = male, 1 = female\n",
    "        # Filter samples for the current profession and gender\n",
    "        subset = train_data[(train_data[\"profession\"] == profession) & (train_data[\"gender\"] == gender)]\n",
    "        \n",
    "        if len(subset) == 0:\n",
    "            continue  # Skip if there are no samples for this combination\n",
    "        \n",
    "        # Oversample or undersample to match max_samples\n",
    "        if len(subset) < max_samples:\n",
    "            oversampled = subset.sample(max_samples, replace=True, random_state=42)\n",
    "            balanced_data.append(oversampled)\n",
    "        else:\n",
    "            undersampled = subset.sample(max_samples, replace=False, random_state=42)\n",
    "            balanced_data.append(undersampled)\n",
    "\n",
    "# Combine the balanced subsets into a single DataFrame\n",
    "balanced_train_data = pd.concat(balanced_data)\n",
    "\n",
    "# Shuffle the data\n",
    "balanced_train_data = balanced_train_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Extract the balanced training data\n",
    "balanced_train_texts = balanced_train_data[\"text\"].tolist()\n",
    "balanced_train_professions = balanced_train_data[\"profession\"].tolist()\n",
    "balanced_train_genders = balanced_train_data[\"gender\"].tolist()\n",
    "\n",
    "print(\"\\nBalanced Training Dataset Created!\")\n",
    "print(f\"Total Samples: {len(balanced_train_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3 (Balanced Data)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 147455/147455 [2:31:49<00:00, 16.19it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Training Loss: 0.1731, Balanced Training Accuracy: 0.9512\n",
      "\n",
      "Epoch 2/3 (Balanced Data)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 147455/147455 [2:31:39<00:00, 16.20it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Training Loss: 0.0240, Balanced Training Accuracy: 0.9932\n",
      "\n",
      "Epoch 3/3 (Balanced Data)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 147455/147455 [2:31:43<00:00, 16.20it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Training Loss: 0.0106, Balanced Training Accuracy: 0.9971\n",
      "\n",
      "Retraining Complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create a new dataset and data loader for the balanced training data\n",
    "balanced_train_dataset = BiasInBiosDataset(balanced_train_texts, balanced_train_professions, tokenizer)\n",
    "balanced_train_loader = DataLoader(balanced_train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# Training parameters\n",
    "epochs = 3\n",
    "learning_rate = 2e-5\n",
    "batch_size = 16\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "\n",
    "# Reinitialize the model\n",
    "model = BertProfessionClassifier(num_classes)\n",
    "model.to(device)\n",
    "\n",
    "# Define a new optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "# Retrain the model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs} (Balanced Data)\")\n",
    "    for batch in tqdm(balanced_train_loader):\n",
    "        # Move data to the same device as the model\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        # Forward pass and compute loss\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    # Scheduler step\n",
    "    scheduler.step()\n",
    "\n",
    "    # Epoch results\n",
    "    avg_loss = total_loss / len(balanced_train_loader)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Balanced Training Loss: {avg_loss:.4f}, Balanced Training Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nRetraining Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Retrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./bert_profession_classifier_retrained\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Define the save directory\n",
    "save_directory = \"./bert_profession_classifier_retrained\"\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Save the model's state dict\n",
    "torch.save(model.state_dict(), os.path.join(save_directory, \"retrained_model.pth\"))\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {save_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load retrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Reinitialize the model\n",
    "save_directory = \"./bert_profession_classifier_retrained\"\n",
    "model = BertProfessionClassifier(num_classes)\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load(os.path.join(save_directory, \"retrained_model.pth\")))\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "model.to(device)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(save_directory)\n",
    "\n",
    "print(\"Model and tokenizer successfully loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Post-Mitigation Bias Detection Results:\n",
      "Male Prompt: He is a teacher. -> Predicted: teacher\n",
      "Female Prompt: She is a teacher. -> Predicted: teacher\n",
      "--------------------------------------------------\n",
      "Male Prompt: He is a nurse. -> Predicted: nurse\n",
      "Female Prompt: She is a nurse. -> Predicted: nurse\n",
      "--------------------------------------------------\n",
      "Male Prompt: He is a software engineer. -> Predicted: software_engineer\n",
      "Female Prompt: She is a software engineer. -> Predicted: software_engineer\n",
      "--------------------------------------------------\n",
      "Male Prompt: He is a surgeon. -> Predicted: journalist\n",
      "Female Prompt: She is a surgeon. -> Predicted: journalist\n",
      "--------------------------------------------------\n",
      "Male Prompt: He is a rapper. -> Predicted: rapper\n",
      "Female Prompt: She is a rapper. -> Predicted: rapper\n",
      "--------------------------------------------------\n",
      "\n",
      "Post-Mitigation Bias Scores:\n",
      "Profession: teacher - Bias Score: 0.0001\n",
      "Profession: nurse - Bias Score: 0.0033\n",
      "Profession: engineer - Bias Score: 0.0001\n",
      "Profession: surgeon - Bias Score: 0.3459\n",
      "Profession: rapper - Bias Score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Repeat bias detection with the retrained model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    male_outputs = model(male_inputs[\"input_ids\"], male_inputs[\"attention_mask\"])\n",
    "    female_outputs = model(female_inputs[\"input_ids\"], female_inputs[\"attention_mask\"])\n",
    "\n",
    "# Convert predictions to probabilities\n",
    "male_probs = torch.nn.functional.softmax(male_outputs, dim=1)\n",
    "female_probs = torch.nn.functional.softmax(female_outputs, dim=1)\n",
    "\n",
    "# Get top predicted classes and probabilities\n",
    "male_predictions = torch.argmax(male_probs, dim=1).cpu().numpy()\n",
    "female_predictions = torch.argmax(female_probs, dim=1).cpu().numpy()\n",
    "\n",
    "# Print predictions for each prompt again\n",
    "print(\"\\nPost-Mitigation Bias Detection Results:\")\n",
    "for i in range(len(male_prompts)):\n",
    "    print(f\"Male Prompt: {male_prompts[i]} -> Predicted: {profession_mapping[male_predictions[i]]}\")\n",
    "    print(f\"Female Prompt: {female_prompts[i]} -> Predicted: {profession_mapping[female_predictions[i]]}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Calculate post-mitigation bias scores\n",
    "bias_scores = []\n",
    "for i in range(len(male_prompts)):\n",
    "    male_prob = male_probs[i, male_predictions[i]].item()\n",
    "    female_prob = female_probs[i, female_predictions[i]].item()\n",
    "    bias_scores.append(abs(male_prob - female_prob))\n",
    "\n",
    "print(\"\\nPost-Mitigation Bias Scores:\")\n",
    "for i in range(len(male_prompts)):\n",
    "    print(f\"Profession: {male_prompts[i].split()[-1][:-1]} - Bias Score: {bias_scores[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class BiasInBiosDataset(Dataset):\n",
    "    def __init__(self, texts, professions, genders, tokenizer, max_length=128):\n",
    "        \"\"\"\n",
    "        Custom dataset for bias mitigation.\n",
    "\n",
    "        Args:\n",
    "            texts (list): List of input texts.\n",
    "            professions (list): List of profession labels (e.g., integers).\n",
    "            genders (list): List of gender labels (e.g., 0 = male, 1 = female).\n",
    "            tokenizer (transformers.PreTrainedTokenizer): Tokenizer to process input texts.\n",
    "            max_length (int): Maximum length for tokenized inputs.\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.professions = professions  # Profession labels\n",
    "        self.genders = genders  # Gender labels (0 = male, 1 = female)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of samples in the dataset.\"\"\"\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a single sample from the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing input_ids, attention_mask, label (profession), and gender.\n",
    "        \"\"\"\n",
    "        # Tokenize the input text\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        # Return input_ids, attention_mask, profession label, and gender label\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),  # Remove batch dimension\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": self.professions[idx],  # Profession label\n",
    "            \"gender\": self.genders[idx],  # Gender label\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assume balanced_train_texts, balanced_train_professions, and balanced_train_genders are already defined\n",
    "# Example:\n",
    "# balanced_train_texts = [\"He is a teacher.\", \"She is a nurse.\", ...]\n",
    "# balanced_train_professions = [0, 1, ...]  # Integer labels for professions\n",
    "# balanced_train_genders = [0, 1, ...]  # 0 for male, 1 for female\n",
    "\n",
    "# Create the dataset\n",
    "balanced_train_dataset = BiasInBiosDataset(\n",
    "    texts=balanced_train_texts,\n",
    "    professions=balanced_train_professions,\n",
    "    genders=balanced_train_genders,  # Pass gender labels\n",
    "    tokenizer=tokenizer,  # Assume tokenizer is already initialized\n",
    ")\n",
    "\n",
    "# Create the data loader\n",
    "balanced_train_loader = DataLoader(\n",
    "    balanced_train_dataset,\n",
    "    batch_size=16,  # Adjust batch size as needed\n",
    "    shuffle=True,\n",
    "    drop_last=True,  # Drop the last incomplete batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 (Adversarial Training)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 147455/147455 [2:27:23<00:00, 16.67it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profession Loss: 25200.8211, Gender Loss: 199.4598\n",
      "Training Accuracy: 0.9518\n",
      "Epoch 2/3 (Adversarial Training)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 147455/147455 [2:28:22<00:00, 16.56it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profession Loss: 7586.6863, Gender Loss: 40.1718\n",
      "Training Accuracy: 0.9848\n",
      "Epoch 3/3 (Adversarial Training)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 147455/147455 [2:27:43<00:00, 16.64it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profession Loss: 5031.7663, Gender Loss: 30.4378\n",
      "Training Accuracy: 0.9900\n",
      "\n",
      "Adversarial Training Complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the adversarial model\n",
    "class AdversarialBERT(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(AdversarialBERT, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)  # For profession classification\n",
    "        self.gender_classifier = nn.Linear(self.bert.config.hidden_size, 2)  # For gender classification\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, gender_labels=None, lambda_adv=0.1):\n",
    "        # Forward pass through BERT\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = self.dropout(outputs.pooler_output)\n",
    "\n",
    "        # Main task: Profession classification\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        # Adversarial task: Gender classification\n",
    "        if gender_labels is not None:\n",
    "            gender_logits = self.gender_classifier(pooled_output)\n",
    "            gender_loss = F.cross_entropy(gender_logits, gender_labels)\n",
    "            return logits, gender_loss * lambda_adv\n",
    "\n",
    "        return logits\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "num_classes = len(set(balanced_train_professions))  # Number of unique professions\n",
    "adv_model = AdversarialBERT(num_classes=num_classes)\n",
    "adv_model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(adv_model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()  # For profession classification\n",
    "lambda_adv = 0.1  # Weight for adversarial loss\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    adv_model.train()\n",
    "    total_loss, total_gender_loss = 0.0, 0.0\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} (Adversarial Training)\")\n",
    "    for batch in tqdm(balanced_train_loader):\n",
    "        # Move data to device\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)  # Profession labels\n",
    "        gender_labels = batch[\"gender\"].to(device)  # Gender labels\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        logits, gender_loss = adv_model(input_ids, attention_mask, gender_labels, lambda_adv)\n",
    "\n",
    "        # Calculate profession classification loss\n",
    "        profession_loss = criterion(logits, labels)\n",
    "        total_loss += profession_loss.item()\n",
    "        total_gender_loss += gender_loss.item()\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        (profession_loss + gender_loss).backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accuracy calculation\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    # Epoch summary\n",
    "    print(f\"Profession Loss: {total_loss:.4f}, Gender Loss: {total_gender_loss:.4f}\")\n",
    "    print(f\"Training Accuracy: {correct / total:.4f}\")\n",
    "\n",
    "print(\"\\nAdversarial Training Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./bert_profession_classifier_retrained_adversarial\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Define the save directory\n",
    "save_directory = \"./bert_profession_classifier_retrained_adversarial\"\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Save the model's state dict\n",
    "torch.save(model.state_dict(), os.path.join(save_directory, \"retrained_model_adversarial.pth\"))\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {save_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Reinitialize the model\n",
    "save_directory = \"./bert_profession_classifier_retrained_adversarial\"\n",
    "model = BertProfessionClassifier(num_classes)\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load(os.path.join(save_directory, \"retrained_model_adversarial.pth\")))\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "model.to(device)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(save_directory)\n",
    "\n",
    "print(\"Model and tokenizer successfully loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bias Mitigation Results:\n",
      "--------------------------------------------------\n",
      "Male Prompt: 'He is a surgeon.'\n",
      "  Predicted Profession: profession_19 | Confidence: 0.5406\n",
      "Female Prompt: 'She is a surgeon.'\n",
      "  Predicted Profession: profession_19 | Confidence: 0.6425\n",
      "Bias Score: 0.1018\n",
      "--------------------------------------------------\n",
      "Male Prompt: 'He is a nurse.'\n",
      "  Predicted Profession: profession_26 | Confidence: 0.7107\n",
      "Female Prompt: 'She is a nurse.'\n",
      "  Predicted Profession: profession_26 | Confidence: 0.7566\n",
      "Bias Score: 0.0459\n",
      "--------------------------------------------------\n",
      "Male Prompt: 'He is a teacher.'\n",
      "  Predicted Profession: profession_26 | Confidence: 0.9984\n",
      "Female Prompt: 'She is a teacher.'\n",
      "  Predicted Profession: profession_26 | Confidence: 0.9980\n",
      "Bias Score: 0.0004\n",
      "--------------------------------------------------\n",
      "Male Prompt: 'He is a software engineer.'\n",
      "  Predicted Profession: profession_24 | Confidence: 0.9998\n",
      "Female Prompt: 'She is a software engineer.'\n",
      "  Predicted Profession: profession_24 | Confidence: 0.9997\n",
      "Bias Score: 0.0001\n",
      "--------------------------------------------------\n",
      "Male Prompt: 'He is a chef.'\n",
      "  Predicted Profession: profession_26 | Confidence: 0.5795\n",
      "Female Prompt: 'She is a chef.'\n",
      "  Predicted Profession: profession_27 | Confidence: 0.5442\n",
      "Bias Score: 0.0352\n",
      "--------------------------------------------------\n",
      "\n",
      "Average Bias Score Across Prompts: 0.0367\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Controlled prompts for evaluation\n",
    "male_prompts = [\n",
    "    \"He is a surgeon.\",\n",
    "    \"He is a nurse.\",\n",
    "    \"He is a teacher.\",\n",
    "    \"He is a software engineer.\",\n",
    "    \"He is a chef.\"\n",
    "]\n",
    "\n",
    "female_prompts = [\n",
    "    \"She is a surgeon.\",\n",
    "    \"She is a nurse.\",\n",
    "    \"She is a teacher.\",\n",
    "    \"She is a software engineer.\",\n",
    "    \"She is a chef.\"\n",
    "]\n",
    "\n",
    "# Tokenize the prompts\n",
    "def tokenize_prompts(prompts, tokenizer, max_length=128):\n",
    "    \"\"\"\n",
    "    Tokenizes a list of prompts for evaluation.\n",
    "\n",
    "    Args:\n",
    "        prompts (list): List of text prompts.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): Tokenizer to process input texts.\n",
    "        max_length (int): Maximum token length.\n",
    "\n",
    "    Returns:\n",
    "        dict: Tokenized inputs with input_ids and attention_mask tensors.\n",
    "    \"\"\"\n",
    "    encodings = tokenizer(\n",
    "        prompts,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": encodings[\"input_ids\"],\n",
    "        \"attention_mask\": encodings[\"attention_mask\"]\n",
    "    }\n",
    "\n",
    "# Tokenize male and female prompts\n",
    "male_inputs = tokenize_prompts(male_prompts, tokenizer)\n",
    "female_inputs = tokenize_prompts(female_prompts, tokenizer)\n",
    "\n",
    "# Move inputs to the device\n",
    "male_inputs = {key: value.to(device) for key, value in male_inputs.items()}\n",
    "female_inputs = {key: value.to(device) for key, value in female_inputs.items()}\n",
    "\n",
    "# Evaluate the adversarially trained model\n",
    "adv_model.eval()  # Set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    # Get predictions for male and female prompts\n",
    "    male_logits = adv_model(male_inputs[\"input_ids\"], male_inputs[\"attention_mask\"])\n",
    "    female_logits = adv_model(female_inputs[\"input_ids\"], female_inputs[\"attention_mask\"])\n",
    "\n",
    "# Convert logits to probabilities using softmax\n",
    "male_probs = F.softmax(male_logits, dim=1)\n",
    "female_probs = F.softmax(female_logits, dim=1)\n",
    "\n",
    "# Get the top predicted classes and their probabilities\n",
    "male_predictions = torch.argmax(male_probs, dim=1).cpu().numpy()\n",
    "female_predictions = torch.argmax(female_probs, dim=1).cpu().numpy()\n",
    "\n",
    "male_confidences = male_probs.max(dim=1).values.cpu().numpy()\n",
    "female_confidences = female_probs.max(dim=1).values.cpu().numpy()\n",
    "\n",
    "# Dynamically handle profession mapping\n",
    "num_classes = male_logits.size(1)  # Number of output classes\n",
    "profession_mapping = {i: f\"profession_{i}\" for i in range(num_classes)}  # Placeholder mapping\n",
    "\n",
    "# Map predictions back to profession labels\n",
    "male_predictions_classes = [profession_mapping[pred] for pred in male_predictions]\n",
    "female_predictions_classes = [profession_mapping[pred] for pred in female_predictions]\n",
    "\n",
    "# Calculate bias scores\n",
    "bias_scores = []\n",
    "for i in range(len(male_prompts)):\n",
    "    bias_score = abs(male_confidences[i] - female_confidences[i])\n",
    "    bias_scores.append(bias_score)\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"\\nBias Mitigation Results:\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(len(male_prompts)):\n",
    "    print(f\"Male Prompt: '{male_prompts[i]}'\")\n",
    "    print(f\"  Predicted Profession: {male_predictions_classes[i]} | Confidence: {male_confidences[i]:.4f}\")\n",
    "    print(f\"Female Prompt: '{female_prompts[i]}'\")\n",
    "    print(f\"  Predicted Profession: {female_predictions_classes[i]} | Confidence: {female_confidences[i]:.4f}\")\n",
    "    print(f\"Bias Score: {bias_scores[i]:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Print overall bias summary\n",
    "average_bias_score = sum(bias_scores) / len(bias_scores)\n",
    "print(f\"\\nAverage Bias Score Across Prompts: {average_bias_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbconvert import ScriptExporter\n",
    "import nbformat\n",
    "\n",
    "# Load the notebook\n",
    "with open('BiasInBios.ipynb', 'r', encoding='utf-8') as f:\n",
    "    notebook = nbformat.read(f, as_version=4)\n",
    "\n",
    "# Convert to script\n",
    "script_exporter = ScriptExporter()\n",
    "script, _ = script_exporter.from_notebook_node(notebook)\n",
    "\n",
    "# Save the script\n",
    "with open('output_script.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(script)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
